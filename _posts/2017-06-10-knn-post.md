---
layout: post
title:  "KNN Algorithm"
date:   2017-06-10
excerpt: ""
tag:
- KNN
- Machine Learning
- Supervised Learning
comments: true
published: true
---
### Introduction
KNN algorithm is a supervised learning algorithm which we store training dataset (labeled) in the training time. And in the test time, we compare distance of the test data with every
every each data in the training dataset, then we select the k (hyperparameter) nearest training examples to classify our test data. The most frequent label of these k training examples, is used to classify
the test data. So the main drawback here is that since there is not actual training (because we just store the data), we need to compare distance with every training data, and as your training data is increasing,
so is your test time (linearly). The other disadvantage is that rgb values are used to measure the distance and this results with color matching, e.g. picture of the airplane on the air mostly have blue color because
of the sky and a ship on the ocean may also have blue background which might be confusing for KNN, you can end up with misclassifying plane as ship or vice versa.
  
There are two distance formulas which are called as L1 (Manhattan distance) and L2 (Euclidean distance) formulas (another hyperparameter);
 
L1 Distance;

$$d(I_i, I_j) = \sum_p|I_i^p - I_j^p|$$

and L2 Distance;

$$d(I_i, I_j) = \sqrt{\sum_p(I_i^p - I_j^p)^2}$$


### Calculating Distance Matrix Without Loops
Let's suppose that we have images with dimensions of 32*32*3, then let's store training dataset (5000 images) in the $$X_{train}$$ matrix,
and test dataset (500 images) in $$X_{test}$$ as follows;

	
<a href="https://github.com/alidemir1/alidemir1.github.io/blob/master/_posts/KNN_1.jpg">
<img src="https://github.com/alidemir1/alidemir1.github.io/blob/master/_posts/KNN_1.jpg"></a>



 